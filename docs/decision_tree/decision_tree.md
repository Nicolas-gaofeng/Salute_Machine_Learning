# 决策树（Decision Tree）

## 1. 决策树概述

决策树算法是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一。我们这章节只讨论用于分类的决策树。

决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。

决策树学习通常包括 3 个步骤: `特征选择`、`决策树的生成`和`决策树的修剪`。

### 决策树的定义

分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型: 内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)。

用决策树对需要测试的实例进行分类: 从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分配到叶结点的类中。

## 2. 决策树场景

一个叫做 "二十个问题" 的游戏，游戏的规则很简单: 参与游戏的一方在脑海中想某个事物，其他参与者向他提问，只允许提 20 个问题，问题的答案也只能用对或错回答。问问题的人通过推断分解，逐步缩小待猜测事物的范围，最后得到游戏的答案。

一个邮件分类系统，大致工作流程如下: 

![image-20210205190431682](https://gitee.com/zgf1366/pic_store/raw/master/img/20210205190431.png)



首先检测发送邮件域名地址。如果地址为 myEmployer.com, 则将其放在分类 "无聊时需要阅读的邮件"中。
如果邮件不是来自这个域名，则检测邮件内容里是否包含单词 "曲棍球" , 如果包含则将邮件归类到 "需要及时处理的朋友邮件", 
如果不包含则将邮件归类到 "无需阅读的垃圾邮件" 。

### 2.1 常见决策树

|    模型    |   ID3    |    C4.5    | CART            |
| :--------: | :------: | :--------: | --------------- |
|    结构    |  多叉树  |   多叉树   | 二叉树          |
|  特征选择  | 信息增益 | 信息增益率 | Gini系数/均方差 |
| 连续值处理 |  不支持  |    支持    | 支持            |
| 缺失值处理 |  不支持  |    支持    | 支持            |
|    枝剪    |  不支持  |    支持    | 支持            |



## 3. 决策树原理

### 3.1 决策树须知概念

##### 3.1.1 信息熵（entropy）

熵指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。

信息论（information theory）中的熵（香农熵）: 
是一种信息的度量方式，表示信息的混乱程度，也就是说: `信息越有序，信息熵越低`。例如: 火柴有序放在火柴盒里，熵值很低，相反，熵值很高。

##### 3.1.2 信息增益（information gain）: 

在划分数据集前后信息发生的变化称为信息增益。

### 3.2 决策树工作原理

如何构造一个决策树?
我们使用 createBranch() 方法，如下所示: 

```python
def createBranch():
'''
此处运用了迭代的思想。 感兴趣可以搜索 迭代 recursion， 甚至是 dynamic programing。
'''
    检测数据集中的所有数据的分类标签是否相同:
        If so return 类标签
        Else:
            寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）
            划分数据集
            创建分支节点
                for 每个划分的子集
                    调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中
            return 分支节点
```

## 4. 决策树开发流程

收集数据: 可以使用任何方法。
准备数据: 树构造算法 (这里使用的是ID3算法，只适用于标称型数据，这就是为什么数值型数据必须离散化。 还有其他的树构造算法，比如CART)
分析数据: 可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
训练算法: 构造树的数据结构。
测试算法: 使用训练好的树计算错误率。
使用算法: 此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。

## 5. 决策树特点

优点: 

- 计算复杂度不高，数据有缺失也能跑，可以处理不相关特征。
- 缺失值不敏感，对特征的宽容程度高，可缺失可连续可离散
- 可解释性强（输出结果易于理解)
- 算法对数据没有强假设
- 可以解决线性及非线性问题
- 有特征选择等辅助功能

缺点: 

- 处理关联性数据比较薄弱
- 正负量级有偏样本的样本效果较差
- 单棵树的拟合效果欠佳，容易过拟合

适用数据类型: 数值型和标称型。

## 6. 决策树算法



## 7. 决策树项目案例

### 6.1 项目案例1: 判定鱼类和非鱼类

#### 6.1.1 项目概述

根据以下 2 个特征，将动物分成两类: 鱼类和非鱼类。

特征: 

1. 不浮出水面是否可以生存
2. 是否有脚蹼

#### 6.1.2 开发流程

[完整代码地址](/src/py2.x/ml/3.DecisionTree/DecisionTree.py): <https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py>

收集数据: 可以使用任何方法
准备数据: 树构造算法（这里使用的是ID3算法，因此数值型数据必须离散化。）
分析数据: 可以使用任何方法，构造树完成之后，我们可以将树画出来。
训练算法: 构造树结构
测试算法: 使用习得的决策树执行分类
使用算法: 此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义


### 6.2 项目案例2: 使用决策树预测隐形眼镜类型

[完整代码地址](/src/py2.x/ml/3.DecisionTree/DecisionTree.py): <https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py>

#### 6.2.1 项目概述

隐形眼镜类型包括硬材质、软材质以及不适合佩戴隐形眼镜。我们需要使用决策树预测患者需要佩戴的隐形眼镜类型。

#### 6.2.2 开发流程

1. 收集数据: 提供的文本文件。
2. 解析数据: 解析 tab 键分隔的数据行
3. 分析数据: 快速检查数据，确保正确地解析数据内容，使用 createPlot() 函数绘制最终的树形图。
4. 训练算法: 使用 createTree() 函数。
5. 测试算法: 编写测试函数验证决策树可以正确分类给定的数据实例。
6. 使用算法: 存储树的数据结构，以便下次使用时无需重新构造树。
