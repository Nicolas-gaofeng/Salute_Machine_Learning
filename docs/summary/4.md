![ml_add_2](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208163110.jpeg)

特征工程是将原始数据转换为更好地代表预测模型的潜在问题的特征的过程，从而提高了对未知数据的模型准确性

 * `特征选择` 

   —— 也叫特征子集选择（FSS，Feature Subset Selection）。是指从已有的 M 个特征（Feature）中选择 N 个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程，是提高算法性能的一个重要手段，也是模式识别中关键的数据预处理步骤。

 * `特征提取` 

   —— 特征提取是计算机视觉和图像处理中的一个概念。它指的是使用计算机提取图像信息，决定每个图像的点是否属于一个图像特征。特征提取的结果是把图像上的点分为不同的子集，这些子集往往属于孤立的点，连续的曲线或者连续的区域。

   `注：特征值化是为了计算机更好的去理解数据`

## 1. 特征抽取

sklearn特征抽取API

-  sklearn.feature_extraction

**连续特征常用方法**

- 截断
    - 连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)
    - 参考异常点里面的outlier识别，以最大值填充或者以None
- 二值化
    - 数据分布过于不平衡
    - 空值/异常值过多
- 分桶
    - 小范围连续数据内不存在逻辑关系，比如31岁和32岁之间不存在明显的差异，可以归为一类
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2hcomdwj30k60djdh4.jpg)
- 离散化    
    - 数值无意义，比如学历、祖籍等等
- 缩放
    - z-score标准化
    - min-max归一化
    - 范数归一化:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mf5xdj2sj3031011jr6.jpg)
        - L1范数
        - L2范数
    - 平方根缩放
    - 对数缩放
        - 对数缩放适用于处理长尾分且取值为正数的数值变量
            - 它将大端长尾压缩为短尾，并将小端进行延伸
        - 可以把类似较差的特征线性化，比如x1x2/y，log变换后变成了log(x1)+log(x2)-log(y)
        - 可以把有偏分布修正为近似正太分布
    - Box-Cox转换
        - ![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8mfjjwir3j309m038gln.jpg)
        - 通过因变量的变换，使得变换后的y(λ)与自变量具有线性依托关系。因此，Box-Cox变换是通过参数的适当选择，达到对原来数据的“综合治理”，使其满足一个线性模型条件
- 特征交叉
    - 人为分段交叉
        - 提升模型的拟合能力，使基向量更有表示能力。比如，本来是在二维空间解释一个点的意义，现在升维到三维后解释
        - 离散变量的交并补
        - 连续变量的点积，attention类似
        - 交叉中需要并行特征筛选的步骤
            - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2rf5aawj30ka0j6q48.jpg)
    - 自动组合
        - FM/FFM中的矩阵点积
            - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2un2ckzj30eb07jaan.jpg)
        - Neural Network里面的dense
            - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2vb1nnij30co07lt94.jpg)
    - 条件选择
        - 通过树或者类似的特征组合模型去做最低熵的特征选择
            - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2t44a2mj30im0bt0tz.jpg)
            - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2v20wjbj30hs0dx0tq.jpg)
    
- 非线性编码
    - 核向量进行升维
    - 树模型的叶子结点的stack
    - 谱聚类/pca/svd等信息抽取编码
    - lda/EM等分布拟合表示

**离散特征常用方法**

- one-hot-encoder
- 分层编码
    - 有一定规律的类别数据，邮政编码，手机号等等
- 计数编码
    - 将类别特征用其对应的计数来代替,这对线性和非线性模型都有效
    - 对异常值比较敏感,特征取值有可能冲突
- 计数排名编码
    - 解决上述问题，以排名代替值
- Embedding
    - 对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题
- 类别特征之间交叉组合
    - 笛卡尔交叉
- 类别特征和数值特征之间交叉组合
    - 均值、中位数、标准差、最大值和最小值
    - 分位数、方差、vif值、分段冲量

### 1.1 字典特征抽取

作用：对字典数据进行特征值化



### 1.2 文本特征抽取

作用：对文本数据进行特征值化

- 预处理手段有哪些？

  - 将字符转化为小写
  - 分词
  - 去除无用字符
  - 繁体转中文
  - 去除停用词
  - 去除稀有词
  - 半角全角切换
  - 错词纠正
  - 关键词标记
    - TF-IDF
    - LDA
    - LSA
  - 提取词根
  - 词干提取
  - 标点符号编码
  - 文档特征
  - 实体插入和提取
  - 文本向量化
    - word2vec
    - glove
    - bert
  - 文本相似性

- 如何做样本构造？

  - 按标点切分
  - 按句切分
  - 对话session切分
  - 按文章切分
  - 按场景切分

- 分词过程中会考虑哪些方面？

  - 词性标注
  - 词形还原和词干提取
    - 词形还原为了通用性特征的提取
    - 词干提取为了去除干扰词把训练注意力集中在关键词上，同时提高速度；缺点是不一定词干代表完整句义

- 文本中的统计信息一般有哪些？

  - 直接统计值：
    - 文本的长度
    - 单词个数
    - 数字个数
    - 字母个数
    - 大小写单词个数
    - 大小写字母个数
    - 标点符号个数
    - 特殊字符个数
    - 数字占比
    - 字母占比
    - 特殊字符占比
    - 不同词性个数    
  - 直接统计值的统计信息：
    - 最小最大均值方差标准差
    - 分位数，最早/最晚出现位置

- 直接对文本特征进行整理手段有哪些？   

  - N-Gram模型
    - 将文本转换为连续序列，扩充样本特征
    - 连续语意的提取
  - TF-IDF
    - 权重评分，去除掉一些低重要性的词，比如每篇文章都出现的"的"，"了"
  - LDA
    - 主题抽取，用狄利克雷分布去拟合出文章和主题之间的关系
  - 相似度
    - 余弦相似度
    - Jaccard相似度
      - 共现性
    - Levenshtein(编辑距离)
      - 文本近似程度
    - 海林格距离
      - 用来衡量概率分布之间的相似性
    - JSD
      - 衡量prob1 和 prob2两个分布的相似程度
  - 向量化
    - word2vec
    - glove
    - bert

  

#### 1.2.1 one-hot 



#### 1.2.2 TF-IDF

TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。

#### 1.2.3 异常点检测

**统计方法**

- 3∂原则
    - 数据需要服从正态分布
    - 只能解决一维问题：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m954lh5tj301h00gdfl.jpg)
- 基于正态分布的离群点检测方法
    - 一元高斯分布校验：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m91basuwj306201h0sl.jpg)，如果概率值大小离群则代表为异常点
    - 多元高斯分布检测：
        - 假设 n 维的数据集合 ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9e4sjkoj303000it8l.jpg)，可以计算 n 维的均值向量![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9ewbb1cj304b00igli.jpg)
        - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9fdr3xfj30140080sl.jpg)的协方差矩阵：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9fly7rpj306g00ja9z.jpg)
        - 得到![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9gzmd1cj30bz01874b.jpg) 
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m92kshc2j30d6073dft.jpg)
- 马氏距离
    - 假设![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9kw2qf3j300900awec.jpg)是均值向量，其中S是协方差矩阵。
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9kguzdgj307e00lmx3.jpg)
- ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9rdqiqjj300g00h0sl.jpg)统计检验
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9ryphxxj304u00kmx2.jpg)
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9sgjwk1j300d00awec.jpg)是a在第i维上的取值,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9sljfhpj300g00e0sl.jpg)是所有对象在第 i 维的均值，n是维度
- 箱型图
    - IQR，\[Q1-3/2(Q3-Q1),Q3+3/2(Q3-Q1)]

**矩阵分解方法**

- PCA
    - 去除均值后的协方差矩阵对应的特征值和特征向量，按照特征值排序，topN个特征向量组成新的低维空间
    - 核心：在于组合原始的特征，使得新的原始数据在新的低维度空间中的方差更大，特征更有区分力
    - 问题是没有做到剔除，只是对空间上的表现进行了优化，尽可能的压缩异常点在新空间中作用
- SVD
    - 假设 dataMat 是一个 p 维的数据集合，有 N 个样本，它的协方差矩阵是 X。那么协方差矩阵就通过奇异值分解写成：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8man1o3trj302h00hmx1.jpg)
    - 其中 P 是一个 (p,p) 维的正交矩阵，它的每一列都是 X 的特征向量。D 是一个 (p,p) 维的对角矩阵，包含了特征值 ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8manhjc7qj301n00ga9x.jpg)。
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mar6eb43j304300h746.jpg)可以认为是dataMat在主成分topj上的映射
    - 最后还需要拉回原空间：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mb1hmrmnj306w00i747.jpg)
    - 异常值分数（outlier score）：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8maynk5wkj30a300mjrc.jpg) + ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mayu64x7j305500mt8m.jpg)

**特征值和特征向量的本质是什么？**

- 一个特征向量可以看成 2 维平面上面的一条线，或者高维空间里面的一个超平面
- 特征向量所对应的特征值反映了这批数据在这个方向上的拉伸程度 

**矩阵乘法的实际意义？**

- 两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。
- 矩阵点乘向量的意义是将右边的向量变换到左边矩阵中每一行行向量为基所表示的空间中去。

**密度的离群点检测**

- 定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。另一种密度定义是使用DBSCAN聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离d内对象的个数。
    - 我们可以通过随机选择联通点，人为设置联通点附近最小半径a，半径内最小容忍点个数b，再考虑密度可达，形成蓝色方框内的正常数据区域，剩下的黄色区域内的点即为异常点。
- Local Outlier Factor算法
- 孤立森林:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mcoswixrj30iy0em0tb.jpg)
    - 经验1：每棵树的最大深度limit length=ceiling(log2(样本大小))
    - 经验2：树的个数在256棵以下
缺点：
    - 计算量大：o(n^2)
    - 需要人为选择阈值
    

**聚类的离群点检测**

- 一个对象是基于聚类的离群点，如果该对象不强属于任何簇，那么该对象属于离群点。
- 缺点也就是聚类的缺点，包括初始点对结果的影响，数据是否保持凸型对结果对影响，簇的个数的选择

**如何处理异常点？**

- 删除含有异常值的记录：直接将含有异常值的记录删除；
- 视为缺失值：将异常值视为缺失值，利用缺失值处理的方法进行处理；
- 平均值修正：可用前后两个观测值的平均值修正该异常值；
- 生成列新特征：category异常
- 不处理：直接在具有异常值的数据集上进行数据挖掘；


## 2. 特征处理

特征处理即通过特定的统计方法（数学方法）将数据转换成算法要求的数据

![image-20210208161022295](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208183307.png)

### 2.1 归一化

特点：通过对原始数据进行变换把数据映射到(默认为[0,1])之间

公式：
$$
X^′= (x-min)/(max-min)
$$

$$
𝑋^′′=𝑋^′∗(𝑚𝑥−𝑚𝑖)+𝑚𝑖
$$

注：作用于每一列，max为一列的最大值，min为一列的最小值,那么X’’为最终结果，mx，mi分别为指定区间值默认mx为1,mi为0

![image-20210208161248859](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208161249.png)

注意在特定场景下最大值最小值是变化的，另外，最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。

### 2.2 标准化

特点：通过对原始数据进行变换把数据变换到均值为0,方差为1范围内



![image-20210208161551990](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208161552.png)

对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变

对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。

**标准化总结**

在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。

**为什么需要对数据进行变换？**

- 避免异常点：比如对连续变量进行份桶离散化
- 可解释性或者需要连续输出：比如评分卡模型中的iv+woe
- 使得原始数据的信息量更大：比如log/sqrt变换

**归一化和标准化之间的关系？**

- 归一化(max-min)
  - 缩放仅仅跟最大、最小值的差别有关，只是一个去量纲的过程

- 标准化(z-score)
  - 缩放和所有点都相关，数据相对分布不会改变，集中的数据标准化后依旧集中

- 作用
  - 解决部分模型由于数据值域不同对模型产生的影响，尤其是距离模型
  - 更快的收敛
  - 去量纲化
  - 避免数值计算溢出

- 总结
  - 异常点角度：特征数据上下限明显异常的时候使用标准化方法，简单归一化会造成数据差异模糊，整体方差下降
  - 分布角度：使用标准化之前，要求数据需要近似满足高斯分布，不然会改变数据的分布，尤其是对数据分布有强假设的情况下
  - 上线变动角度：归一化在上线的时候需要考虑上下约束届是否需要变动，标准化则不需要考虑变动
  - 值域范围角度：归一化对数据范围约定较为固定，而标准化的输出上下届则不定
  - 模型角度：一般涉及距离计算，协方差计算，数据满足高斯分布的情况下用标准化，其他归一化或其他变换

### 2.3 缺失值

删除 - 如果每列或者行数据缺失值达到一定的比例，建议放弃整行或者整列

插补 - 可以通过缺失值每行或者每列的平均值、中位数来填充

**是不是一定需要对缺失值处理？**

当缺失值占比在可接受的范围以内的时候才需要进行填充，如果缺失值大于50%以上的时候，可以选择进行二分化，如果缺失值大于80%可以完整删除该列而不是强行去填充

**直接填充方法有哪些？**

- 均值
- 中位数
- 众数
- 分位数

**模型插值方法有哪些？及方法的问题**

- 有效性存疑，取决于特征列数
  - 生成的插值来源于其他列的特征，是不是意味着插值的结果已经是和其他列的组合高相关

**如何直接离散化？**

- 离散特征新增缺失的category

**hold位填充方法有哪些？**

把全部结果都embedding化，对空值或者缺失值按照一定规则生成若干个hold位，以hold位的向量结果作为缺失值的结果
    - 可以参考YouTube中的新商品向量生成逻辑
        - bert中的\[UNK]向量，\[unused]向量

**怎么理解分布补全？**

如果我们能在原始数据上发现明显规律，比如整体数据满足高维多元高斯分布，则可以通过未知列补全缺失列的值

**random方法**

在缺失量特别少(通常认为小于1%)的时候，可以随机生成

**总结**

- 实际机器学习工程中，直接删除、众数填充和直接离散化方法用的最多
  - 快速
  - 对原始数据的前提假设最少，也不会影响到非缺失列
- 在深度学习中，hold位填充方法用的最多
  - 在大量数据的拟合条件下，能保证这些未知数据处的向量也能得到收敛
  - 而且通过随机构造的特性，保证了缺失处的\[UNK]向量，\[unused]向量的通配性

## 3. 特征选择

### 3.1 特征选择原因

- 冗余：部分特征的相关度高，容易消耗计算性能
- 噪声：部分特征对预测结果有负影响
- 耗时：特征个数越多，分析特征、训练模型所需的时间就越长。
- 过拟合：特征个数越多，容易引起“维度灾难”，模型也会越复杂，其推广能力会下降。    
- 共线性：单因子对目标的作用被稀释，解释力下降

### 3.2 特征选择是什么

特征选择就是单纯地从提取到的所有特征中选择部分特征作为训练集特征，特征在选择前和选择后可以改变值、也不改变值，但是选择后的特征维数肯定比选择前小，毕竟我们只选择了其中的一部分特征。

主要方法（三大武器）：

Filter(过滤式):VarianceThreshold

Embedded(嵌入式)：正则化、决策树

Wrapper(包裹式)

### 3.3 从哪些方面可以做特征选择？

- 方差，是的feature内的方向更大，对目标区分度提高更高贡献
  - 移除低方差特征
    - 移除低方差特征是指移除那些方差低于某个阈值，即特征值变动幅度小于某个范围的特征，这一部分特征的区分度较差，我们进行移除
  - 考虑有值数据中的占比，异常数据的占比，正常范围数据过少的数据也可以移除

- 相关性，与区分目标相关的特征才有意义
  - 单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况
    - 皮尔森相关系数:![](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208192254.jpeg)
    - Fisher得分:

![](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208192322.jpeg)



- 假设检验
  - 卡方检验
  - ANOVA

- 熵检验
  - 互信息熵
    - 度量两个变量之间的相关性,互信息越大表明两个变量相关性越高;互信息为0,两个变量越独立
  - KL散度
  - 相对熵

## 4. 数据平衡

**为什么要对数据进行采样平衡**

- 下采样：克服高维特征以及大量数据导致的问题,有助于降低成本,缩短时间甚至提升效果
- 上采样：均衡正负样本的数据，避免数据不平衡导致分类器对正负样本的有偏训练
  - 比如99%为正样本，1%为负样本，如果分类器把所以样本预测为正样本则准确率高达99%，显然不符合实际情况

**是否一定需要对原始数据进行采样平衡**

否。

- 采样前后会对原始数据的分布进行改变，可能导致泛化能力大大下降
- 采样有一定概率会造成过拟合，当原始数据过少而采样量又很大当时候，造成大量数据被重复，造成模型训练的结果有一定的过拟合

**有哪些常见的采样方法？**

- 随机采样
  - 无放回的简单抽样：每条样本被采到的概率相等且都为1/N
  - 有放回的简单抽样：每条样本可能多次被选中
  - 上采样：即合理地增加少数类的样本
  - 下采样：欠抽样技术是将数据从原始数据集中移除
  - 平衡采样：考虑正负样本比
  - 分层采样：通过某一些feature对数据进行切分，按照切分后的占比分别进行采样
  - 整体采样：先将数据集T中的数据分组成G个互斥的簇,然后再从G个簇中简单随机采样s个簇作为样本集
- 合成采样
  相对于采样随机的方法进行上采样, 还有两种比较流行的上采样的改进方式： 
    - SMOTE
      - x_new = x + rand(0,1) * (x′−x)
      - **带来新样本的同时有可能造成不同类别样本之间的重合**
      - Borderline-SMOTE为了解决上面的问题，在x_new生成之前，会先判断x这个点是否周围都是同类别的点
    - ADASYN
      - 同上，也是在构造样本点的过程中考虑了正负样本比
- 平衡欠采样        
  - EasyEnsemble，利用模型融合的方法（Ensemble）
    - 少样本不变，多样本拆分成N份，分别组合进行模型训练后进行模型融合
  - BalanceCascade，利用模型融合的方法（Boost）
    - 每次剔除预测正确的多数样本，加入新的未预测的多数样本
  - NearMiss
    - 选择离各种情况下的少数样本位置最远的多数样本进行训练                

**能否避免采样？**

可以通过修改模型训练中的loss权重，比如逻辑回归中进行case_weight的调整，adaboost中对样本错分权重的改变等等。

**怎么用采样方法？**

尽量避免使用合成采样的方式去做数据填充，总结如下：

- 由于项目中时间的充裕问题，填充的结果往往是正负样本交叠且无感知的，会干扰分类器
- 通常我们引入的特征不仅仅是连续变量，在分类变量上合成采样表现并不优秀
- 合成采样往往无法与后序模型进行结合使用，比如随机采样可以引入模型交叉，比如平衡欠采样可以引入模型融合