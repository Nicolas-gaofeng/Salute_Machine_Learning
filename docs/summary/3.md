## 1. 误差

我们将学习器对样本的实际预测结果与样本的真实值之间的差异成为：误差（error）。定义：	

 - 在训练集上的误差称为`训练误差（training error）`或`经验误差（empirical error）`。
 - 在测试集上的误差称为`测试误差（test error）`。
 - 学习器在所有新样本上的误差称为`泛化误差（generalization error）`。

### 1.1 泛化误差上界

泛化误差上界可理解为模型学习能力的“出错上限”，显然，当样本容量趋于无穷大时，泛化误差上界趋于0.

本文介绍较简单的二分类问题中的泛化误差上界.以下先给出结论：

在二分类问题中，若假设空间为有限个函数的集合
$$
\mathcal{F}=\left\{f_{1}, f_{2}, \cdots, f_{d}\right\}
$$
，
对于任意一个函数
$$
f \in \mathcal{F}
$$
，至少以概率1−δ,

以下不等式成立：
$$
R(f) \leqslant \hat{R}(f)+\varepsilon(d, N, \delta)
$$
其中，
$$
R(f)=E[L(Y, f(X))]
$$

$$
\hat{R}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)
$$

$$
\varepsilon(d, N, \delta)=\sqrt{\frac{1}{2 N}\left(\log d+\log \frac{1}{\delta}\right)}
$$



## 2. 拟合程度

显然，我们希望得到的是在新样本上表现得很好的学习器，即`泛化误差小`的学习器。因此，我们应该让学习器尽可能地从训练集中学出普适性的“一般特征”，这样在遇到新样本时才能做出正确的判别。然而，当学习器把训练集学得“太好”的时候，即把一些训练样本的自身特点当做了普遍特征；同时也有学习能力不足的情况，即训练集的基本特征都没有学习出来。我们定义：

 - `过拟合（overfitting）`：学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了。
    - 训练误差十分小，但测试误差教大；模型把训练样本学习“太好了”，可能把一些训练样本自身的特性当做了所有潜在样本都有的一般性质，导致泛化能力下降。类比，做课后题全都做对了，超纲题也都认为是考试必考题目，上了考场还是啥都不会。 但过拟合问题还没有十分好的解决方案，过拟合是机器学习面临的关键障碍。

 - `欠拟合（underfitting）`：学习能太差，训练样本的一般性质尚未学好。
    - 训练误差和测试误差都比较大。模型没有很好地捕捉到数据特征，不能够很好地拟合数据，对训练样本的一般性质尚未学好。类比，光看书不做题觉得自己什么都会了，上了考场才知道自己啥都不会。目前，欠拟合问题比较容易克服，例如增加迭代次数等

> 通俗来说，欠拟合和过拟合都可以用一句话来说，欠拟合就是: “你太天真了！”，过拟合就是: “你想太多了！”。

![image-20210131150435347](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131150435.png)

## 3. 数据集

在现实任务中，我们往往有多种算法可供选择，那么我们应该选择哪一个算法才是最适合的呢？我们希望得到的是泛化误差小的学习器，理想的解决方案是对模型的泛化误差进行评估，然后选择泛化误差最小的那个学习器。但是，泛化误差指的是模型在所有新样本上的适用能力，我们无法直接获得泛化误差。

因此，通常我们采用一个“测试集”来测试学习器对新样本的判别能力，然后以“测试集”上的“测试误差”作为“泛化误差”的近似。显然：我们选取的测试集应尽可能与训练集互斥，下面用一个小故事来解释why：

假设老师出了10 道习题供同学们练习，考试时老师又用同样的这10道题作为试题，可能有的童鞋只会做这10 道题却能得高分，很明显：这个考试成绩并不能有效地反映出真实水平。回到我们的问题上来，我们希望得到泛化性能好的模型，好比希望同学们课程学得好并获得了对所学知识"举一反三"的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。显然，若测试样本被用作训练了，则得到的将是过于"乐观"的估计结果。

如上所述：我们希望用一个“测试集”的“测试误差”来作为“泛化误差”的近似，因此我们需要对初始数据集进行有效划分，划分出互斥的“训练集”和“测试集”。

样本集: 训练数据 + 测试数据

* 训练样本 = 特征(feature) + 目标变量(label: 分类-离散值/回归-连续值)
* 特征通常是训练样本集的列，它们是独立测量得到的。
* 目标变量: 目标变量是机器学习预测算法的测试结果。
  * 在分类算法中目标变量的类型通常是标称型(如: 真与假)，而在回归算法中通常是连续型(如: 1~100)。

以[鸢尾花数据集](https://en.wikipedia.org/wiki/lris_flower_data_set)为例： 

![image-20210131124216034](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131124216.png)

下面是鸢尾花的数据：

![img](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131124233.png)

- 数据整体叫数据集（data set）
- 每一行数据称为一个样本（sample）
- 除最后一列，每一列表达样本的一个特征（feature）
- 最后一列，称为标记（label）

第i个样本行写作![img](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131124351.png) ，也叫特征向量。第i个样本第j个特征值![img](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131124406.png) 第i个样本的标记写作![img](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131124419.png)

为了可视化特征方便，我们只抽取出特征中的前两个特征，其中萼片的长度作为横轴，萼片的宽度作为纵轴。

绘制下图：

![img](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131124337.png)

对于每一个样本来说都会在坐标系中表示一个点，假设我们有三个特征，就可以在三维空间中表示它，同理如果有1000种特征，就可以在1000维的空间中表示它，而这个绘制样本的空间我们称它为**特征空间(feature space)**。

通过可视化绘制样本点后，我们可以比较轻易的绘制出一根直线，红色样本在直线的一边而蓝色样本在直线的另一边。

**分类任务本质就是在特征空间切分，在高维空间同理。**

而鸢尾花拥有4个特征，应该是在4维特征空间中分析。

> 另外，特征可以很抽象

![img](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131124736.png)

- 图像，每一个像素点都是特征
- 28*28的图像有28*28=784个特征
- 如果是彩色图像特征更多

### 3.1 数据集一般划分

 * `训练集（Training set）` 

   —— 学习样本数据集，通过匹配一些参数来建立一个模型，主要用来训练模型。类比考研前做的解题大全。

 * `验证集（validation set）` 

   —— 对学习出来的模型，调整模型的参数，如在神经网络中选择隐藏单元数。验证集还用来确定网络结构或者控制模型复杂程度的参数。类比 考研之前做的模拟考试。

 * `测试集（Test set）`

   —— 测试训练好的模型的分辨能力。类比 考研。这次真的是一考定终身。

### 3.2 留出法

将数据集D划分为两个互斥的集合，一个作为训练集S，一个作为测试集T，满足D=S∪T且S∩T=∅，常见的划分为：大约2/3-4/5的样本用作训练，剩下的用作测试。需要注意的是：训练/测试集的划分要尽可能保持数据分布的一致性，以避免由于分布的差异引入额外的偏差，常见的做法是采取分层抽样。同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。

### 3.3 交叉验证法

将数据集D划分为k个大小相同的互斥子集，满足D=D1∪D2∪...∪Dk，Di∩Dj=∅（i≠j），同样地尽可能保持数据分布的一致性，即采用分层抽样的方法获得这些子集。交叉验证法的思想是：每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就有K种训练集/测试集划分的情况，从而可进行k次训练和测试，最终返回k次测试结果的均值。交叉验证法也称“`k折交叉验证`”，k最常用的取值是10，下图给出了10折交叉验证的示意图。

![image-20210131151114643](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131151114.png)

与留出法类似，将数据集D划分为K个子集的过程具有随机性，因此K折交叉验证通常也要重复p次，称为p次k折交叉验证，常见的是10次10折交叉验证，即进行了100次训练/测试。特殊地当划分的k个子集的每个子集中只有一个样本时，称为“留一法”，显然，留一法的评估结果比较准确，但对计算机的消耗也是巨大的。

### 3.4 自助法

我们希望评估的是用整个D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。“自助法”正是解决了这样的问题。

自助法的基本思想是：给定包含m个样本的数据集D，每次随机从D 中挑选一个样本，将其拷贝放入D'，然后再将该样本放回初始数据集D 中，使得该样本在下次采样时仍有可能被采到。重复执行m 次，就可以得到了包含m个样本的数据集D'。可以得知在m次采样中，样本始终不被采到的概率取极限为：

$$
\lim _{m \mapsto \infty}\left(1-\frac{1}{m}\right)^{m} \mapsto \frac{1}{e} \approx 0.368
$$
这样，通过自助采样，初始样本集D中大约有36.8%的样本没有出现在D'中，于是可以将D'作为训练集，D-D'作为测试集。自助法在数据集较小，难以有效划分训练集/测试集时很有用，但由于自助法产生的数据集（随机抽样）改变了初始数据集的分布，因此引入了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用。

## 4. 调参

大多数学习算法都有些参数(parameter) 需要设定，参数配置不同，学得模型的性能往往有显著差别，这就是通常所说的"参数调节"或简称"调参" (parameter tuning)。

学习算法的很多参数是在实数范围内取值，因此，对每种参数取值都训练出模型来是不可行的。常用的做法是：对每个参数选定一个范围和步长λ，这样使得学习的过程变得可行。例如：假定算法有3 个参数，每个参数仅考虑5 个候选值，这样对每一组训练/测试集就有5*5*5= 125 个模型需考察，由此可见：拿下一个参数（即经验值）对于算法人员来说是有多么的happy。

最后需要注意的是：当选定好模型和调参完成后，我们需要使用初始的数据集D重新训练模型，即让最初划分出来用于评估的测试集也被模型学习，增强模型的学习效果。

## 5. 性能度量(performance measure)

在上一篇中，我们解决了评估学习器泛化性能的方法，即用测试集的“测试误差”作为“泛化误差”的近似，当我们划分好训练/测试集后，那如何计算“测试误差”呢？这就是性能度量，例如：均方差，错误率等，即“测试误差”的一个评价标准。有了评估方法和性能度量，就可以计算出学习器的“测试误差”，但由于“测试误差”受到很多因素的影响，例如：算法随机性或测试集本身的选择，那如何对两个或多个学习器的性能度量结果做比较呢？这就是比较检验。最后偏差与方差是解释学习器泛化性能的一种重要工具。

性能度量（performance measure）是衡量模型泛化能力的评价标准，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。

### 5.1 常见的性能度量

在回归任务中，即预测连续值的问题，最常用的性能度量是“均方误差”（mean squared error）,很多的经典算法都是采用了MSE作为评价函数。
$$
E(f ; D)=\frac{1}{m} \sum_{i=1}^{m}\left(f\left(\boldsymbol{x}_{i}\right)-y_{i}\right)^{2}
$$

$$
\text { 更一般的, 对于数据分布 } \mathcal{D} \text { 和概率密度函数 } p(\cdot), \text { 均方误差可描述为 }
$$

$$
E(f ; \mathcal{D})=\int_{\boldsymbol{x} \sim \mathcal{D}}(f(\boldsymbol{x})-y)^{2} p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}
$$

在分类任务中，即预测离散值的问题，最常用的是错误率和精度，错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例，易知：错误率+精度=1。

错误率定义为
$$
E(f ; D)=\frac{1}{m} \sum_{i=1}^{m} \mathbb{I}\left(f\left(\boldsymbol{x}_{i}\right) \neq y_{i}\right)
$$
精度定义为
$$
\begin{aligned}
\operatorname{acc}(f ; D) &=\frac{1}{m} \sum_{i=1}^{m} \mathbb{I}\left(f\left(\boldsymbol{x}_{i}\right)=y_{i}\right) \\
&=1-E(f ; D)
\end{aligned}
$$
更一般的，对于数据分布D和概率密度函数P（·），错误率和精度可分别描述为
$$
E(f ; \mathcal{D})=\int_{\boldsymbol{x} \sim \mathcal{D}} \mathbb{I}(f(\boldsymbol{x}) \neq y) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}
$$


### 5.2 精确率、召回率、F1

错误率和精度虽然常用，但不能满足所有的需求，例如：在推荐系统中，我们只关心推送给用户的内容用户是否感兴趣（即查准率），或者说所有用户感兴趣的内容我们推送出来了多少（即查全率）。因此，使用查准/查全率更适合描述这类问题。对于二分类问题，分类结果混淆矩阵与查准/查全率定义如下：

![image-20210131195245619](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131195245.png)

 * `精确率（precision）` 

   —— 提取出的正确信息条数 / 提取出的信息条数
   $$
   P=\frac{T P}{T P+F P}
   $$
   
 * `召回率（recall）` 

   —— 提取出的正确信息条数 / 样本中的信息条数
   $$
   R=\frac{T P}{T P+F N}
   $$
   
 * `F1值` 

   —— 正确率 * 召回率 * 2 / （正确率 + 召回率）（F值即为正确率和召回率的调和平均值）
   $$
   F 1=\frac{2 \times P \times R}{P+R}=\frac{2 \times T P}{\text { 样例总数 }+T P-T N}
   $$
   ![image-20210131165803863](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131165803.png)

举个例子如下: 
某池塘有 1400 条鲤鱼，300 只虾，300 只乌龟。现在以捕鲤鱼为目的。撒了一张网，逮住了 700 条鲤鱼，200 只
虾， 100 只乌龟。那么这些指标分别如下: 
正确率 = 700 / (700 + 200 + 100) = 70%
召回率 = 700 / 1400 = 50%
F 值 = 70% * 50% * 2 / (70% + 50%) = 58.3%



- 正如天下没有免费的午餐，正确率和召回率是一对矛盾的度量。一般来说，正确率高时，召回率往往偏低;而召回率高时，正确率往往偏低。例如我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容，这样就漏掉了一些用户感兴趣的内容，查全率就低了；如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，宁可错杀一千，不可放过一个，这样查准率就很低了。通常只有在一些简单任务中才可能使召回率和正确率都很高。

![image-20210131165552491](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131165552.png)

P-R曲线如何评估呢？若一个学习器A的P-R曲线被另一个学习器B的P-R曲线完全包住，则称：B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，所以衍生出了“平衡点”（Break-Event Point，简称BEP），即当P=R时的取值，平衡点的取值越高，性能更优。

P和R指标有时会出现矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的加权调和平均。

### 5.3 宏查准率(macro-P)、宏查全率(macro-R)、宏F1

有很多时候我们有多个二分类混淆矩阵，我们希望在n个二分类混淆矩阵上综合考察查准率和查全率。一种直接的做法是先在各混淆矩阵上分别计算出查准率和查全率再计算平均值，这样就得到宏查准率(macro-P)、宏查全率(macro-R),以及相应的宏F1
$$
\operatorname{macro}-P=\frac{1}{n} \sum_{i=1}^{n} P_{i}
$$

$$
\operatorname{macro}-R=\frac{1}{n} \sum_{i=1}^{n} R_{i}
$$

$$
\text { macro- } F 1=\frac{2 \times \operatorname{macro}-P \times \operatorname{macro}-R}{\operatorname{macro}-P+\operatorname{macro}-R}
$$

### 5.4 ROC与AUC

#### 5.4.1 ROC曲线

##### 5.4.1.1 ROC的动机

　　对于0，1两类分类问题，一些分类器得到的结果往往不是0，1这样的标签，如神经网络得到诸如0.5，0.8这样的分类结果。这时，我们人为取一个阈值，比如0.4，那么小于0.4的归为0类，大于等于0.4的归为1类，可以得到一个分类结果。同样，这个阈值我们可以取0.1或0.2等等。取不同的阈值，最后得到的分类情况也就不同。如下面这幅图：

![img](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131192339.png)

蓝色表示原始为负类分类得到的统计图，红色表示原始为正类得到的统计图。那么我们取一条直线，直线左边分为负类，直线右边分为正类，这条直线也就是我们所取的阈值。阈值不同，可以得到不同的结果，但是由分类器决定的统计图始终是不变的。这时候就需要一个独立于阈值，只与分类器有关的评价指标，来衡量特定分类器的好坏。还有在类不平衡的情况下，如正样本有90个，负样本有10个，直接把所有样本分类为正样本，得到识别率为90%，但这显然是没有意义的。如上就是ROC曲线的动机。

##### 5.4.1.2 ROC的定义

　　关于两类分类问题，原始类为positive、negative，分类后的类别为p'、n'。排列组合后得到4种结果，如下图所示：

![img](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131192436.png)

　于是我们得到四个指标，分别为：真阳、伪阳、伪阴、真阴。ROC空间将伪阳性率（FPR）定义为 X 轴，真阳性率（TPR）定义为 Y 轴。这两个值由上面四个值计算得到，公式如下：

　　TPR：在所有实际为阳性的样本中，被正确地判断为阳性之比率。TPR=TP/(TP+FN)

　　FPR：在所有实际为阴性的样本中，被错误地判断为阳性之比率。FPR=FP/(FP+TN)

　　放在具体领域来理解上述两个指标。如在医学诊断中，判断有病的样本。那么尽量把有病的揪出来是主要任务，也就是第一个指标TPR，要越高越好。而把没病的样本误诊为有病的，也就是第二个指标FPR，要越低越好。不难发现，这两个指标之间是相互制约的。如果某个医生对于有病的症状比较敏感，稍微的小症状都判断为有病，那么他的第一个指标应该会很高，但是第二个指标也就相应地变高。最极端的情况下，他把所有的样本都看做有病，那么第一个指标达到1，第二个指标也为1。

##### 5.4.1.3 ROC的图形化表示

　　我们以FPR为横轴，TPR为纵轴，得到如下ROC空间：

![788753-20161121105420346-41033633](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131192714.png)

我们可以看出：左上角的点（TPR=1，FPR=0），为完美分类，也就是这个医生医术高明，诊断全对；点A（TPR>FPR），医生A的判断大体是正确的。中线上的点B（TPR=FPR），也就是医生B全都是蒙的，蒙对一半，蒙错一半；下半平面的点C（TPR<FPR），这个医生说你有病，那么你很可能没有病，医生C的话我们要反着听，为真庸医。

上图中一个阈值，得到一个点。现在我们需要一个独立于阈值的评价指标来衡量这个医生的医术如何，也就是遍历所有的阈值，得到ROC曲线。还是一开始的那幅图，假设如下就是某个医生的诊断统计图，直线代表阈值。我们遍历所有的阈值，能够在ROC平面上得到如下的ROC曲线。

![img](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131192803.png)

曲线距离左上角越近，证明分类器效果越好。

![788753-20161121105504034-991875038](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131192908.png)

如上，是三条ROC曲线，在0.23处取一条直线。那么，在同样的FPR=0.23的情况下，红色分类器得到更高的TPR。也就表明，ROC越往上，分类器效果越好。我们用一个标量值AUC来量化他。

#### 5.4.2 AUC值

##### 5.4.2.1 AUC值的定义

　　AUC值为ROC曲线所覆盖的区域面积，显然，AUC越大，分类器分类效果越好。

　　AUC = 1，是完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。

　　0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。

　　AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。

　　AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。

##### 5.4.2.2 AUC值的物理意义

　　假设分类器的输出是样本属于正类的socre（置信度），则AUC的物理意义为，任取一对（正、负）样本，正样本的score大于负样本的score的概率。

##### 5.4.2.3 AUC值的计算

（1）第一种方法：AUC为ROC曲线下的面积，那我们直接计算面积可得。面积为一个个小的梯形面积之和，计算的精度与阈值的精度有关。

（2）第二种方法：根据AUC的物理意义，我们计算正样本score大于负样本的score的概率。取N*M（N为正样本数，M为负样本数）个二元组，比较score，最后得到AUC。时间复杂度为O(N*M)。

（3）第三种方法：与第二种方法相似，直接计算正样本score大于负样本的score的概率。我们首先把所有样本按照score排序，依次用rank表示他们，如最大score的样本，rank=n(n=N+M)，其次为n-1。那么对于正样本中rank最大的样本（rank_max），有M-1个其他正样本比他score小，那么就有(rank_max-1)-(M-1)个负样本比他score小。其次为(rank_second-1)-(M-2)。最后我们得到正样本大于负样本的概率为：
$$
\frac{\sum_{\text {所有正样本 }} \operatorname{rank}-\mathrm{M}(\mathrm{M}+1) / 2}{M * N}
$$
时间复杂度为O(N+M)。

### 5.5 代价敏感错误率与代价曲线

上面的方法中，将学习器的犯错同等对待，但在现实生活中，将正例预测成假例与将假例预测成正例的代价常常是不一样的，例如：将无疾病-->有疾病只是增多了检查，但有疾病-->无疾病却是增加了生命危险。以二分类为例，由此引入了“代价矩阵”（cost matrix）。

![image-20210131195852432](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131195852.png)

在非均等错误代价下，我们希望的是最小化“总体代价”，这样“代价敏感”的错误率为：
$$
E(f ; D ; c o s t)=\frac{1}{m}\left(\sum_{x_{i} \in D^{+}} \mathbb{I}\left(f\left(x_{i}\right) \neq y_{i}\right) \times \cos t_{01}+\sum_{x_{i} \in D^{-}} \mathbb{I}\left(f\left(x_{i}\right) \neq y_{i}\right) \times \operatorname{cost}_{10}\right)
$$
同样对于ROC曲线，在非均等错误代价下，演变成了“代价曲线”，代价曲线横轴是取值在[0,1]之间的正例概率代价，式中p表示正例的概率，纵轴是取值为[0,1]的归一化代价。
$$
P(+) \cos t=\frac{p \times \operatorname{cost}_{01}}{p \times \operatorname{cost}_{01}+(1-p) \times \operatorname{cost}_{10}}
$$

$$
c o s t_{n o r m}=\frac{F N R \times p \times \cos t_{01}+F P R \times(1-p) \times \operatorname{cost}_{10}}{p \times \cos t_{01}+(1-p) \times \cos t_{10}}
$$

代价曲线的绘制很简单：设ROC曲线上一点的坐标为(TPR，FPR) ，则可相应计算出FNR，然后在代价平面上绘制一条从(0，FPR) 到(1，FNR) 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将ROC 曲线土的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价，如图所示：

![image-20210131200152578](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131200152.png)

## 6. 比较检验

由于“测试误差”受到很多因素的影响，例如：算法随机性(例如常见的K-Means)或测试集本身的选择，使得同一模型每次得到的结果不尽相同，同时测试误差是作为泛化误差的近似，并不能代表学习器真实的泛化性能，那如何对单个或多个学习器在不同或相同测试集上的性能度量结果做比较呢？这就是比较检验。

在比较学习器泛化性能的过程中，统计假设检验（hypothesis test）为学习器性能比较提供了重要依据，即若A在某测试集上的性能优于B，那A学习器比B好的把握有多大。 为方便论述，本篇中都是以“错误率”作为性能度量的标准。

### 6.1 假设检验

“假设”指的是对样本总体的分布或已知分布中某个参数值的一种猜想，例如：假设总体服从泊松分布，或假设正态总体的期望u=u0。回到本篇中，我们可以通过测试获得测试错误率，但直观上测试错误率和泛化错误率相差不会太远，因此可以通过测试错误率来推测泛化错误率的分布，这就是一种假设检验。

![1.png](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208185741.png)

![2.png](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208185751.png)

![3.png](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208185759.png)

### 6.2 交叉验证t检验

![4.png](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208185823.png)

### 6.3 McNemar检验

MaNemar主要用于二分类问题，与成对t检验一样也是用于比较两个学习器的性能大小。主要思想是：若两学习器的性能相同，则A预测正确B预测错误数应等于B预测错误A预测正确数，即e01=e10，且|e01-e10|服从N（1，e01+e10）分布。

![5.png](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208185844.png)

因此，如下所示的变量服从自由度为1的卡方分布，即服从标准正态分布N（0,1）的随机变量的平方和，下式只有一个变量，故自由度为1，检验的方法同上：做出假设-->求出满足显著度的临界点-->给出拒绝域-->验证假设。

![6.png](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208185858.png)

### 6.4 Friedman检验与Nemenyi后续检验

上述的三种检验都只能在一组数据集上，F检验则可以在多组数据集进行多个学习器性能的比较，基本思想是在同一组数据集上，根据测试结果（例：测试错误率）对学习器的性能进行排序，赋予序值1,2,3...，相同则平分序值，如下图所示：

![7.png](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208185919.png)

若学习器的性能相同，则它们的平均序值应该相同，且第i个算法的平均序值ri服从正态分布N（（k+1）/2，（k+1）(k-1)/12），则有：

![8.png](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208185930.png)



![9.png](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208185936.png)

服从自由度为k-1和(k-1)(N-1)的F分布。下面是F检验常用的临界值：

![10.png](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208185950.png)

若“H0：所有算法的性能相同”这个假设被拒绝，则需要进行后续检验，来得到具体的算法之间的差异。常用的就是Nemenyi后续检验。Nemenyi检验计算出平均序值差别的临界值域，下表是常用的qa值，若两个算法的平均序值差超出了临界值域CD，则相应的置信度1-α拒绝“两个算法性能相同”的假设。

![11.png](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208190002.png)

![12.png](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208190006.png)

## 7. 偏差与方差

偏差-方差分解是解释学习器泛化性能的重要工具。

- 偏差：

期望值与真实值之间的一致差距，衡量的是学习器预测的**准确性**

- 方差:

期望值与真实值之间的波动程度，衡量的是学习器预测的**稳定性**

通过对泛化误差的进行分解，可以得到：

 + **期望泛化误差=方差+偏差**	
 + **偏差刻画学习器的拟合能力**
 + **方差体现学习器的稳定性**

易知：方差和偏差具有矛盾性，这就是常说的偏差-方差窘境（bias-variance dilamma），随着训练程度的提升，期望预测值与真实值之间的差异越来越小，即偏差越来越小，但是另一方面，随着训练程度加大，学习算法对数据集的波动越来越敏感，方差值越来越大。换句话说：在欠拟合时，偏差主导泛化误差，而训练到一定程度后，偏差越来越小，方差主导了泛化误差。因此训练也不要贪杯，适度辄止。

![13.png](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208190033.png)

### 7.1 高方差

什么情况下引发高方差？

- 过高复杂度的模型，对训练集进行过拟合
  - 带来的后果就是在训练集合上效果非常好，但是在校验集合上效果极差
  - 更加形象的理解就是用一条高次方程去拟合线性数据

如何解决高方差问题？

- 在模型复杂程度不变的情况下，增加更多数据
- 在数据量不变的情况下，减少特征维度
- 在数据和模型都不变的情况下，加入正则化

以上方法是否一定有效？

- 增加数据如果和原数据分布一致，无论增加多少必定解决不了高方差
  - smote对样本进行扩充是否必定可以避免高方差？
  - 过采样是否解决高方差问题？
- 减少的特征维度如果是共线性的维度，对原模型没有任何影响
  - 罗辑回归中，如果把一列特征重复2遍，会对最后的结果产生影响么？
- 正则化通常都是有效的

### 7.2 高偏差

如何解决高偏差问题？

- 尝试获得更多的特征
  - 从数据入手，进行特征交叉，或者特征的embedding化
- 尝试增加多项式特征
  - 从模型入手，增加更多线性及非线性变化，提高模型的复杂度
- 尝试减少正则化程度λ

以上方法是否一定有效？

- 特征越稀疏，高方差的风险越高
- 多个线性变换=一个线性变换，多个非线性变换不一定=一个多线性变换
- 正则化通常都是有效的

### 7.3 模型训练为什么要引入偏差和方差？

优化监督学习=优化模型的泛化误差，模型的泛化误差可分解为偏差、方差与噪声之和
**Err = bias + var + irreducible error** 

以回归任务为例,其实更准确的公式为：**Err = bias^2 + var + irreducible error^2** 

符号的定义：一个真实的任务可以理解为Y=f(x)+e，其中f(x)为规律部分，e为噪声部分

### 7.4 Bagging、Boosting的方差偏差问题

- 从偏差-方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树，神经网络等易受样本扰动的学习器上效果更为明显。
- 从偏差-方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。

- - bagging和boosting都要n个模型，假设基模型权重![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyh07lb3j300a00c0ok.jpg)，相关系数![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyi3r97uj300900c0oe.jpg)，方差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzqhhkfwj300h00g0rq.jpg)均相等
  - Var(x,y) = Var(x) + Var(y) + 2Cov(x,y)
- Bagging
  - Var(F) = ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyk4wjxkj302v00qa9u.jpg)
        = ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyp35ynkj308l00qglh.jpg)
    - 其中
      - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lypqw1tjj300d00c0q9.jpg)可以直接提取出来
      - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyuaep07j308v00ya9x.jpg)
    - 所以，化简以上的式子可得：Var(F) = m * ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyw59y9oj300h00g0rq.jpg) * ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lywg4870j300g00k0r2.jpg) + ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyzygc4lj304300lgle.jpg)
    - 以上为通式，对于bagging来说，每个基模型的权重等于1/m且期望近似相等，所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz1clt7wj301g011gld.jpg)，带入即可
    - Var(F) = ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz7qrpwsj304f015t8i.jpg)
    - E(F) = ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz97enuuj3059011743.jpg)
  - 结论：
    - 整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似
    - 整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高
    - bagging的防止过拟合的极限在1/m项趋近于0，所以并不是可以无穷的降低方差达到提高模型准确性的效果的

- Boosting 同理
  - boosting的前提是弱模型之间高度相关，我们不妨设相关度为1
  - Var(F) = ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzwvy93dj302k00kmwx.jpg)
  - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzj06hv1j304300qwea.jpg)
  - 结论：
    - 整体模型的期望近似于基模型的期望之和，模型越多期望越容易拟合真实值
    - 整体模型的方差等于基模型的数量平方成正比，越多模型不稳定性越高，越容易过拟合。
    - Gradient Boosting Decision Tree为典型例子
