## 一、机器学习概述

![img](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131125111.png)

`机器学习(Machine Learning,ML)` 是使用计算机来彰显数据背后的真实含义，它为了把无序的数据转换成有用的信息。

机器学习是从数据中自动分析获得规律（模型），并利用规律对未知数据进行预测。

它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，**它主要使用归纳、综合而不是演绎。**

需明确几点问题：

算法是核心，数据和计算是基础

## 二、机器学习研究意义

机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能”。 “机器学习是对能通过经验自动改进的计算机算法的研究”。 “机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。” 一种经常引用的英文定义是: A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.

机器学习已应用于多个领域，横跨: 计算机科学、工程技术和统计学等多个学科。有了十分广泛的应用，例如: 数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人运用。

![image-20210208152417343](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208152435.png)

* 搜索引擎: 根据你的搜索点击，优化你下次的搜索结果,是机器学习来帮助搜索引擎判断哪个结果更适合你（也判断哪个广告更适合你）。
* 垃圾邮件: 会自动的过滤垃圾广告邮件到垃圾箱内。
* 超市优惠券: 你会发现，你在购买小孩子尿布的时候，售货员会赠送你一张优惠券可以兑换6罐啤酒。
* 邮局邮寄: 手写软件自动识别寄送贺卡的地址。
* 申请贷款: 通过你最近的金融活动信息进行综合评定，决定你是否合格。

## 三、机器学习使用场景

![image-20210208152045392](https://gitee.com/zgf1366/pic_store/raw/master/img/20210208152045.png)

### 3.1 什么时候机器可以学习？

- 事物存在某种潜在规律
- 某些问题难以使用普通编程解决
- 有大量数据可供使用

### 3.2 最早的机器学习应用

垃圾邮件分辨
传统的计算机解决问题思路：

- 编写规则，定义“垃圾邮件”，让计算机执行
- 对于很多问题，规则很难定义
- 规则在不断变化

### 3.3 机器学习识别动物猫

![image-20210131122750516](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131122757.png)

a. **`模式识别`（官方标准）: 人们通过大量的经验，得到结论，从而判断它就是猫。**

> 模式识别（pattern recognition）: 模式识别是最古老的（作为一个术语而言，可以说是很过时的）。

* 我们把环境与客体统称为“模式”，识别是对模式的一种认知，是如何让一个计算机程序去做一些看起来很“智能”的事情。
* 通过融于智慧和直觉后，通过构建程序，识别一些事物，而不是人，例如: 识别数字。

b. **`机器学习`（数据学习）: 人们通过阅读进行学习，观察它会叫、小眼睛、两只耳朵、四条腿、一条尾巴，得到结论，从而判断它就是猫。**

> 机器学习（machine learning）: 机器学习是最基础的（当下初创公司和研究实验室的热点领域之一）。

* 在90年代初，人们开始意识到一种可以更有效地构建模式识别算法的方法，那就是用数据（可以通过廉价劳动力采集获得）去替换专家（具有很多图像方面知识的人）。
* “机器学习”强调的是，在给计算机程序（或者机器）输入一些数据后，它必须做一些事情，那就是学习这些数据，而这个学习的步骤是明确的。
* 机器学习（Machine Learning）是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身性能的学科。

c. **`深度学习`（深入数据）: 人们通过深入了解它，发现它会'喵喵'的叫、与同类的猫科动物很类似，得到结论，从而判断它就是猫。（深度学习常用领域: 语音识别、图像识别）**

> 深度学习（deep learning）: 深度学习是非常崭新和有影响力的前沿领域，我们甚至不会去思考-后深度学习时代。

* 深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。

### 3.4 选择算法需要考虑的问题

#### 3.4.1 算法场景

* 预测明天是否下雨，因为可以用历史的天气情况做预测，所以选择监督学习算法
* 给一群陌生的人进行分组，但是我们并没有这些人的类别信息，所以选择无监督学习算法、通过他们身高、体重等特征进行处理。

#### 3.4.2 需要收集或分析的数据是什么

- 举例

![机器学习基础-选择算法](https://gitee.com/zgf1366/pic_store/raw/master/img/20210130213037.jpg)

数据类型

• 离散型数据：

由记录不同类别个体的数目所得到的数据，又称计数数据，所有这些数据全部都是整数，而且不能再细分，也不能进一步提高他们的精确度。

• 连续型数据：

变量可以在某个范围内取任一数，即变量的取值可以是连续的，如，长度、时间、质量值等，这类整数通常是非整数，含有小数部分。

注：只要记住一点，离散型是区间内不可分，连续型是区间内可分

## 四、机器学习划分

### 4.1 按照输入空间划分

- `Concrete Features`
- `Raw Features`
- `Abstract Features`



### 4.2 按不同协议划分

- `Batch Learning`
- `Online Learning`
- `Active Learning`



### 4.3 按输出空间划分

![ml_add_1](https://gitee.com/zgf1366/pic_store/raw/master/img/20210130213058.jpg)

- `分类问题`

> 分类（classification）: 将实例数据划分到合适的类别中。

* 说白了就是将一些未知类别的数据分到现在已知的类别中去。比如，根据你的一些信息，判断你是高富帅，还是穷屌丝。
* 评判分类效果好坏的三个指标: 正确率，召回率，F值。
* 应用实例: 判断网站是否被黑客入侵（二分类 ），手写数字的自动识别（多分类）

![img](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131124958.png)

- 有很过看似不是分类的任务可以转换为分类任务，例如，利用机器学习玩2048游戏，将玩游戏转换为在当前状态下是进行上移、下移、左移还是右移。又比如利用机器学习玩围棋，自动驾驶等等童谣可以转换成分类任务来。

![img](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131125041.png)



- `回归问题`

> 回归（regression）: 主要用于预测数值型数据。

* 回归问题 —— 对数值型连续随机变量进行预测和建模的监督学习算法。
* 结果是一个连续数字的值，而非一个类别
* 回归往往会通过计算 误差（Error）来确定模型的精确性。
* 应用实例: 股票价格波动的预测，房屋价格的预测等。
* 对于回归任务来说，有些算法只能够解决回归问题；有一些算法只能够解决分类；有一些算法的思路既能够解决回归问题，又能解决分类问题。一些情况下，回归任务可以简化成分类任务。比如，预测学生成绩，我们可能并不需要预测学生的具体成绩而只需要预测学生分数是A、B、C还是D就足够了，那么此时就是一个分类任务。



- `聚类问题`

> 聚类是一种无监督学习任务，该算法基于数据的内部结构寻找观察样本的自然族群（即集群）。

- 聚类问题的标准一般基于距离: 簇内距离（Intra-cluster Distance） 和 簇间距离（Inter-cluster Distance） 。簇内距离是越小越好，也就是簇内的元素越相似越好；而簇间距离越大越好，也就是说簇间（不同簇）元素越不相同越好。
- 一般的，衡量聚类问题会给出一个结合簇内距离和簇间距离的公式。

### 4.4 按样本标签划分

- `监督式学习（supervised learning）`

> 必须确定目标变量的值，以便机器学习算法可以发现特征和目标变量之间的关系。在监督学习中，给定一组数据，我们知道正确的输出结果应该是什么样子，并且知道在输入和输出之间有着一个特定的关系。 (包括: 分类和回归)

通俗的讲，监督学习就是给机器的训练数据拥有“标记”或者“答案”，例如：

![img](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131125825.png)

我们需要告诉机器左边的画面是一只狗，而右边的照片是一只猫。同理对于MNIST数据集，给机器图像信息后还应该附上标记信息，如图所示：

![img](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131125841.png)

- 监督学习常见算法
  - 分类
    - K-近邻算法
    - 决策树
    - 朴素贝叶斯算法
    - 线性回归
    - 局部加权线性回归
    - Ridege回归
    - Lasso最小回归系数估计
    - Logistic回归
    - 支持向量机
    - AdaBoost
  - 回归
    - 线性回归
    - 树回归

* 监督学习需要注意的问题: 
  * 偏置方差权衡
  * 功能的复杂性和数量的训练数据
  * 输入空间的维数
  * 噪声中的输出值
* 知识表示: 
  * 可以采用规则集的形式【例如: 数学成绩大于90分为优秀】
  * 可以采用概率分布的形式【例如: 通过统计分布发现，90%的同学数学成绩，在70分以下，那么大于70分定为优秀】
  * 可以使用训练样本集中的一个实例【例如: 通过样本集合，我们训练出一个模型实例，得出 年轻，数学成绩中高等，谈吐优雅，我们认为是优秀】

- `半监督式学习`

一部分数据有“标记”或者“答案”，另一部分没有

相对监督学习，更常见的是各种原因产生的标记缺失的半监督学习。

通常都先使用无监督学习手段对数据做处理，之后使用监督学习手段作模型的训练和预测。

- `无监督式学习（unsupervised learning）`

> 在机器学习，无监督学习的问题是，在未加标签的数据中，试图找到隐藏的结构。因为提供给学习者的实例是未标记的，因此没有错误或报酬信号来评估潜在的解决方案。
>
> 无监督学习是密切相关的统计数据密度估计的问题。然而无监督学习还包括寻求，总结和解释数据的主要特点等诸多技术。在无监督学习使用的许多方法是基于用于处理数据的数据挖掘方法。
>
> 数据没有类别信息，也不会给定目标值。

通俗的讲，无监督学习就是给机器训练数据没有任何“标记”或者“答案”

![img](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131125953.png)

* 无监督学习包括的类型: 
  * 聚类: 在无监督学习中，将数据集分成由类似的对象组成多个类的过程称为聚类。
  * 密度估计: 通过样本分布的紧密程度，来估计与分组的相似性。
  * 此外，无监督学习还可以减少数据特征的维度，以便我们可以使用二维或三维图形更加直观地展示数据信息。

- 无监督学习常见算法
  - K-均值聚类
  - 最大期望算法
  - DBSCAN
  - Parzen窗设计
  - 异常检测：如图所示：图中两个红点明显与其他点脱离，如果它们同属与一种数据，我们可以将这两个点归类为异常，将其去除。当突然图中为二维点，在高维中我们会使用相应的算法剔除异常数据。
  
  ![img](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131130019.png)
- `强化学习`

> 这个算法可以训练程序做出某一决定。程序在某一情况下尝试所有的可能行动，记录不同行动的结果并试着找出最好的一次尝试来做决定。 
>

- 根据周围环境的情况，采取行动，根据采取行动的结果，学习行动方式。

![img](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131130129.png)

- 强化学习常见算法
  - 马尔可夫决策过程
- 训练过程

![机器学习基础训练过程](https://gitee.com/zgf1366/pic_store/raw/master/img/20210130213001.jpg)

### 4.5 其他划分

- `在线学习 （Online Learning）`

![image-20210131131012795](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131131012.png)

优点：及时反映新的环境变化
问题：新的数据带来不好的变化？
解决方案：需要加强对数据进行监控
其他：也适用于数据量巨大，完全无法批量学习的环境。

- `批量学习（离线学习）（Batch Learning）`

![image-20210131130751386](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131130751.png)

优点：简单
问题：如何适应环境变化？
解决方案：定时重新批量学习
缺点：每次重新批量学习，运算量巨大；在某些环境变化非常快的情况下，甚至不可能的。

- `参数学习`

![image-20210131131145618](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131131145.png)


$$
f(x)=a^{\star} x+b
$$
在模型学习过程中不断调整 a 和 b

一旦学习到了参数，就不在需要原有的数据集

- `非参数学习`
  - 不对模型进行过多假设
  - 非参数不等于没参数



## 五、机器学习开发流程

机器学习不仅仅是调库，但是不反对调库。在调库的时候应该对概念原理了解。

深入代码内部，可以帮助我们更好的理解算法。

更好的理解算法，可以帮助我们更好的选择算法，甚至在将来创造新的算法。

进行机器学习的一般步骤包括以下：

![image-20210131140221351](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131140221.png)

### 5.1 收集数据

- 收集样本数据

### 5.2 准备数据

- 注意数据的格式

### 5.3 分析数据

为了确保数据集中没有垃圾数据；

* 如果是算法可以处理的数据格式或可信任的数据源，则可以跳过该步骤；
* 另外该步骤需要人工干预，会降低自动化系统的价值。

### 5.4 训练算法

[机器学习算法核心]

确定模型的假设空间，即学习的策略

如果使用无监督学习算法，由于不存在目标变量值，则可以跳过该步骤

### 5.5 测试算法

[机器学习算法核心]评估算法效果

### 5.6 使用算法

将机器学习算法转为应用程序，利用学习的最优模型对新数据进行预测或分析

## 六、机器学习数学基础

* 微积分
* 统计学/概率论
* [线性代数](https://nicolas-gaofeng.github.io/Salute_Math/#/linear_algebra/chapter01)

## 七、机器学习工具

### 7.1 Python语言 

1. 可执行伪代码
2. Python比较流行: 使用广泛、代码范例多、丰富模块库，开发周期短
5. Python相关的库
   * 科学函数库: `SciPy`、`NumPy`(底层语言: C和Fortran)
   * 绘图工具库: `Matplotlib`
   * 数据分析库 `Pandas`

### 7.2 数学工具

* Matlab

### 7.3 其他

框架：Scikit-learn

numpy，matplotlib，Jupyter notebook ...

## 八、机器学习哲学思考

### 数据即算法？

机器学习与大数据的结合产生了巨大的价值。基于机器学习技术的发展，数据能够“预测”。对人类而言，积累的经验越丰富，阅历也广泛，对未来的判断越准确。例如常说的“经验丰富”的人比“初出茅庐”的小伙子更有工作上的优势，就在于经验丰富的人获得的规律比他人更准确。而在机器学习领域，根据著名的一个实验，有效的证实了机器学习界一个理论：即机器学习模型的数据越多，机器学习的预测的效率就越好。见下图：

![111](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131132327.png)

通过这张图可以看出，各种不同算法在输入的数据量达到一定级数后，都有相近的高准确度。于是诞生了机器学习界的名言：成功的机器学习应用不是拥有较好的算法，而是拥有最多的数据！

### 算法为王？

![image-20210131132545116](https://gitee.com/zgf1366/pic_store/raw/master/img/20210131132545.png)

alphaGo本身没有太多的数据，而数据都是靠算法产生的，这就说明算法本身也很重要

### 奥卡姆的剃刀

机器学习领域众多的算法如何选择？--简单地就是好的

奥卡姆剃刀原理：一种最基本的归纳偏好，即 “若有多个假设与观察一致，则选最简单那个”。

### 没有免费的午餐定理

**（No Free Lunch Theorem， 简称 NFL 定理）**

算法在训练集之外的所有样本上的误差为：
$$
E_{\text {ote }}\left(\xi_{a} \mid X, f\right)=\sum_{h} \sum_{x \in \chi-X} P(x) \mathbb{I}(h(x) \neq f(x)) P\left(h \mid X, \xi_{a}\right)
$$
对于所有可能的 f 按均匀分布求和，则有：
$$
\begin{aligned}
\sum_{f} E_{\text {ote }}\left(\xi_{a} \mid X, f\right) &=\sum_{f} \sum_{h} \sum_{x \in \chi-X} P(x) \mathbb{I}(h(x) \neq f(x)) P\left(h \mid X, \xi_{a}\right) \\
&=2^{|\chi|-1} \sum_{x \in \chi-X} P(x) \cdot 1
\end{aligned}
$$
可以严格地数学推导出：任意两个算法，总误差与学习算法无关！

也就是说，无论学习算法好坏与否，它们的期望性能都相同！但是我们需要知道上述定理论述过程中假设了 f 的均匀分布，而实际情况可能并非如此。实际运用中，某些假设可能是不符合实际甚至根本不存在的。所以，NFL 定理并非是要让我们认为机器学习算法没有用处，而是要让我们认识到讨论算法要**结合实际**才有意义，脱离实际谈论什么算法更好毫无意义可言。

- 没有一种算法，绝对比另一种算法好
- 脱离具体问题，谈那个算法好是没有意义的
- 在面对一个具体问题的时候，尝试使用多种算法进行对比试验，是必要的。

## 九、机器学习专业术语

* 模型（model）: 计算机层面的认知
* 学习算法（learning algorithm），从数据中产生模型的方法
* 数据集（data set）: 一组记录的合集
* 示例（instance）: 对于某个对象的描述
* 样本（sample）: 也叫示例
* 属性（attribute）: 对象的某方面表现或特征
* 特征（feature）: 同属性
* 属性值（attribute value）: 属性上的取值
* 属性空间（attribute space）: 属性张成的空间
* 样本空间/输入空间（samplespace）: 同属性空间
* 特征向量（feature vector）: 在属性空间里每个点对应一个坐标向量，把一个示例称作特征向量
* 维数（dimensionality）: 描述样本参数的个数（也就是空间是几维的）
* 学习（learning）/训练（training）: 从数据中学得模型
* 训练数据（training data）: 训练过程中用到的数据
* 训练样本（training sample）:训练用到的每个样本
* 训练集（training set）: 训练样本组成的集合
* 假设（hypothesis）: 学习模型对应了关于数据的某种潜在规则
* 真相（ground-truth）:真正存在的潜在规律
* 学习器（learner）: 模型的另一种叫法，把学习算法在给定数据和参数空间的实例化
* 预测（prediction）: 判断一个东西的属性
* 标记（label）: 关于示例的结果信息，比如我是一个“好人”。
* 样例（example）: 拥有标记的示例
* 标记空间/输出空间（label space）: 所有标记的集合
* 分类（classification）: 预测是离散值，比如把人分为好人和坏人之类的学习任务
* 回归（regression）: 预测值是连续值，比如你的好人程度达到了0.9，0.6之类的
* 二分类（binary classification）: 只涉及两个类别的分类任务
* 正类（positive class）: 二分类里的一个
* 反类（negative class）: 二分类里的另外一个
* 多分类（multi-class classification）: 涉及多个类别的分类
* 测试（testing）: 学习到模型之后对样本进行预测的过程
* 测试样本（testing sample）: 被预测的样本
* 聚类（clustering）: 把训练集中的对象分为若干组
* 簇（cluster）: 每一个组叫簇
* 监督学习（supervised learning）: 典范--分类和回归
* 无监督学习（unsupervised learning）: 典范--聚类
* 未见示例（unseen instance）: “新样本“，没训练过的样本
* 泛化（generalization）能力: 学得的模型适用于新样本的能力
* 分布（distribution）: 样本空间的全体样本服从的一种规律
* 独立同分布（independent and identically distributed，简称i,i,d.）:获得的每个样本都是独立地从这个分布上采样获得的。
